<html>
<head>
<title>Running Spark with Docker Swarm on DigitalOcean </title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>与Docker Swarm一起在数字海洋上运行火花</h1>
<blockquote>原文：<a href="https://testdriven.io/blog/running-spark-with-docker-swarm-on-digitalocean/#0001-01-01">https://testdriven.io/blog/running-spark-with-docker-swarm-on-digitalocean/#0001-01-01</a></blockquote><div><div class="blog-content long-content" data-local-nav-source="">
    <p>让我们看看如何将用于大规模数据处理的开源集群计算框架Apache Spark部署到T2数字海洋(DigitalOcean)上的Docker Swarm集群。我们还将了解如何根据需要自动配置(和取消配置)机器，以降低成本。</p>



<h2 id="project-setup">项目设置</h2>
<p>克隆项目回购:</p>
<div class="codehilite"><pre><span/><code>$ git clone https://github.com/testdrivenio/spark-docker-swarm
$ <span class="nb">cd</span> spark-docker-swarm
</code></pre></div>

<p>然后，从<a href="https://hub.docker.com/r/mjhea0/spark/"> Docker Hub </a>中拉出预建的<code>spark</code>图像:</p>
<div class="codehilite"><pre><span/><code>$ docker pull mjhea0/spark:3.0.2
</code></pre></div>

<blockquote>
<p>Spark版本2.0.1，2.3.3，2.4.1也有<a href="https://hub.docker.com/r/mjhea0/spark/tags">可用</a>。</p>
</blockquote>
<p>该图像大小约为800MB，因此下载可能需要几分钟时间，这取决于您的连接速度。在等待它完成时，请随意查看用于构建该图像的<a href="https://github.com/testdrivenio/spark-docker-swarm/blob/master/Dockerfile"> Dockerfile </a>以及<a href="https://github.com/testdrivenio/spark-docker-swarm/blob/master/count.py"> count.py </a>，我们将通过Spark运行它。</p>
<p>一旦提取，将<code>SPARK_PUBLIC_DNS</code>环境变量设置为<code>localhost</code>或Docker机器的IP地址:</p>
<div class="codehilite"><pre><span/><code>$ <span class="nb">export</span> <span class="nv">EXTERNAL_IP</span><span class="o">=</span>localhost
</code></pre></div>

<blockquote>
<p><code>SPARK_PUBLIC_DNS</code>设置Spark主机和工作机的公共DNS名称。</p>
</blockquote>
<p>点燃容器:</p>
<div class="codehilite"><pre><span/><code>$ docker-compose up -d --build
</code></pre></div>

<p>这将旋转火花主人和一个工人。在浏览器中导航到Spark master的web UI，网址为<a href="http://localhost:8080"> http://localhost:8080 </a>:</p>
<p><a href="/static/images/blog/spark-docker-swarm/web_ui_1.png"><img data-src="/static/images/blog/spark-docker-swarm/web_ui_1.png" loading="lazy" class="lazyload" alt="spark web ui" src="../Images/bdc8da9904724acbfb4d0b93bda5ccce.png" data-original-src="https://testdriven.io/static/images/blog/spark-docker-swarm/web_ui_1.png"/>T2】</a></p>
<p>要启动Spark工作，我们需要:</p>
<ol>
<li>获取<code>master</code>服务的容器ID，并将其分配给一个名为<code>CONTAINER_ID</code>的环境变量</li>
<li>将<em> count.py </em>文件复制到<code>master</code>容器中的“/tmp”目录</li>
<li>运行作业！</li>
</ol>
<p>尝试一下:</p>
<div class="codehilite"><pre><span/><code><span class="c1"># get container id, assign to env variable</span>
$ <span class="nb">export</span> <span class="nv">CONTAINER_ID</span><span class="o">=</span><span class="k">$(</span>docker ps --filter <span class="nv">name</span><span class="o">=</span>master --format <span class="s2">"{{.ID}}"</span><span class="k">)</span>

<span class="c1"># copy count.py</span>
$ docker cp count.py <span class="nv">$CONTAINER_ID</span>:/tmp

<span class="c1"># run spark</span>
$ docker <span class="nb">exec</span> <span class="nv">$CONTAINER_ID</span> <span class="se">\</span>
  bin/spark-submit <span class="se">\</span>
    --master spark://master:7077 <span class="se">\</span>
    --class endpoint <span class="se">\</span>
    /tmp/count.py
</code></pre></div>

<p>跳回Spark master的web UI。您应该会看到一个正在运行的作业:</p>
<p><a href="/static/images/blog/spark-docker-swarm/web_ui_2.png"><img data-src="/static/images/blog/spark-docker-swarm/web_ui_2.png" loading="lazy" class="lazyload" alt="spark web ui" src="../Images/7d7663b0e0f5cb134225442962ba9e87.png" data-original-src="https://testdriven.io/static/images/blog/spark-docker-swarm/web_ui_2.png"/>T2】</a></p>
<p>在终端中，您应该会看到输出的火花日志。如果一切顺利，来自<em> counts.py </em>的<code>get_counts()</code>函数的输出应该是:</p>


<p><a href="/static/images/blog/spark-docker-swarm/output_1.png"><img data-src="/static/images/blog/spark-docker-swarm/output_1.png" loading="lazy" class="lazyload" alt="spark terminal output" src="../Images/bf41c52e6dacc66414ba1f9516051b59.png" data-original-src="https://testdriven.io/static/images/blog/spark-docker-swarm/output_1.png"/>T2】</a></p>
<p>有了这个，让我们来旋转一个蜂群吧！</p>
<h2 id="docker-swarm">码头工人群</h2>
<p>首先，你需要<a href="https://m.do.co/c/d8f211a4b4c2">注册</a>一个数字海洋账户(如果你还没有的话)，然后<a href="https://www.digitalocean.com/docs/apis-clis/api/create-personal-access-token/">生成</a>一个访问令牌，这样你就可以访问<a href="https://developers.digitalocean.com/documentation/v2/">数字海洋API </a>。</p>
<p>将令牌添加到您的环境中:</p>
<div class="codehilite"><pre><span/><code>$ <span class="nb">export</span> <span class="nv">DIGITAL_OCEAN_ACCESS_TOKEN</span><span class="o">=[</span>your_digital_ocean_token<span class="o">]</span>
</code></pre></div>

<p>旋转三个数字海洋液滴:</p>
<div class="codehilite"><pre><span/><code>$ <span class="k">for</span> i <span class="k">in</span> <span class="m">1</span> <span class="m">2</span> <span class="m">3</span><span class="p">;</span> <span class="k">do</span>
    docker-machine create <span class="se">\</span>
      --driver digitalocean <span class="se">\</span>
      --digitalocean-access-token <span class="nv">$DIGITAL_OCEAN_ACCESS_TOKEN</span> <span class="se">\</span>
      --engine-install-url <span class="s2">"https://releases.rancher.com/install-docker/19.03.9.sh"</span> <span class="se">\</span>
      node-<span class="nv">$i</span><span class="p">;</span>
  <span class="k">done</span>
</code></pre></div>

<p>在<code>node-1</code>初始化<a href="https://docs.docker.com/engine/swarm/">群模式</a>:</p>
<div class="codehilite"><pre><span/><code>$ docker-machine ssh node-1 <span class="se">\</span>
  -- docker swarm init <span class="se">\</span>
  --advertise-addr <span class="k">$(</span>docker-machine ip node-1<span class="k">)</span>
</code></pre></div>

<p>从上一个命令的输出中获取join令牌，然后将剩余的节点作为workers添加到群中:</p>
<div class="codehilite"><pre><span/><code>$ <span class="k">for</span> i <span class="k">in</span> <span class="m">2</span> <span class="m">3</span><span class="p">;</span> <span class="k">do</span>
    docker-machine ssh node-<span class="nv">$i</span> <span class="se">\</span>
      -- docker swarm join --token YOUR_JOIN_TOKEN<span class="p">;</span>
  <span class="k">done</span>
</code></pre></div>

<p>耗尽蜂群管理器:</p>
<div class="codehilite"><pre><span/><code>$ docker-machine ssh node-1 -- docker node update --availability drain node-1
</code></pre></div>

<blockquote>
<p>清空群管理器使其不能运行任何容器是一个好的做法。</p>
</blockquote>
<p>将Docker守护进程指向<code>node-1</code>，更新<code>EXTERNAL_IP</code>环境变量，并部署堆栈:</p>
<div class="codehilite"><pre><span/><code>$ <span class="nb">eval</span> <span class="k">$(</span>docker-machine env node-1<span class="k">)</span>
$ <span class="nb">export</span> <span class="nv">EXTERNAL_IP</span><span class="o">=</span><span class="k">$(</span>docker-machine ip node-2<span class="k">)</span>
$ docker stack deploy --compose-file<span class="o">=</span>docker-compose.yml spark
</code></pre></div>

<p>添加另一个工作节点:</p>
<div class="codehilite"><pre><span/><code>$ docker service scale <span class="nv">spark_worker</span><span class="o">=</span><span class="m">2</span>
</code></pre></div>

<p>查看堆栈:</p>


<p>您应该会看到类似如下的内容:</p>
<div class="codehilite"><pre><span/><code>ID             NAME             IMAGE                NODE      DESIRED STATE   CURRENT STATE
uoz26a2zhpoh   spark_master.1   mjhea0/spark:3.0.2   node-3    Running         Running <span class="m">23</span> seconds ago
ek7j1imsgvjy   spark_worker.1   mjhea0/spark:3.0.2   node-2    Running         Running <span class="m">21</span> seconds ago
l7jz5s29rqrc   spark_worker.2   mjhea0/spark:3.0.2   node-3    Running         Running <span class="m">24</span> seconds ago
</code></pre></div>

<p>将Docker守护进程指向Spark master所在的节点:</p>
<div class="codehilite"><pre><span/><code>$ <span class="nv">NODE</span><span class="o">=</span><span class="k">$(</span>docker service ps --format <span class="s2">"{{.Node}}"</span> spark_master<span class="k">)</span>
$ <span class="nb">eval</span> <span class="k">$(</span>docker-machine env <span class="nv">$NODE</span><span class="k">)</span>
</code></pre></div>

<p>获取IP:</p>
<div class="codehilite"><pre><span/><code>$ docker-machine ip <span class="nv">$NODE</span>
</code></pre></div>

<p>确保Spark master的web UI在<a href="http://YOUR_MACHINE_IP:8080">http://YOUR _ MACHINE _ IP:8080</a>打开。您还应该看到两个工人:</p>
<p><a href="/static/images/blog/spark-docker-swarm/web_ui_3.png"><img data-src="/static/images/blog/spark-docker-swarm/web_ui_3.png" loading="lazy" class="lazyload" alt="spark web ui" src="../Images/171b54bf997dc5e2e8b017f1f4f5ab13.png" data-original-src="https://testdriven.io/static/images/blog/spark-docker-swarm/web_ui_3.png"/>T2】</a></p>
<p>获取Spark master的容器ID，并将其设置为环境变量:</p>
<div class="codehilite"><pre><span/><code>$ <span class="nb">export</span> <span class="nv">CONTAINER_ID</span><span class="o">=</span><span class="k">$(</span>docker ps --filter <span class="nv">name</span><span class="o">=</span>master --format <span class="s2">"{{.ID}}"</span><span class="k">)</span>
</code></pre></div>

<p>复制文件:</p>
<div class="codehilite"><pre><span/><code>$ docker cp count.py <span class="nv">$CONTAINER_ID</span>:/tmp
</code></pre></div>

<p>测试:</p>
<div class="codehilite"><pre><span/><code>$ docker <span class="nb">exec</span> <span class="nv">$CONTAINER_ID</span> <span class="se">\</span>
  bin/spark-submit <span class="se">\</span>
    --master spark://master:7077 <span class="se">\</span>
    --class endpoint <span class="se">\</span>
    /tmp/count.py
</code></pre></div>

<p>同样，您应该在Spark master的web UI中看到作业运行，同时在终端中看到输出的Spark日志。</p>
<p><a href="/static/images/blog/spark-docker-swarm/web_ui_4.png"><img data-src="/static/images/blog/spark-docker-swarm/web_ui_4.png" loading="lazy" class="lazyload" alt="spark web ui" src="../Images/0c4de321e41af73a04c3a7761029514d.png" data-original-src="https://testdriven.io/static/images/blog/spark-docker-swarm/web_ui_4.png"/>T2】</a></p>
<p>作业完成后关闭节点:</p>
<div class="codehilite"><pre><span/><code>$ docker-machine rm node-1 node-2 node-3 -y
</code></pre></div>

<h2 id="automation-scripts">自动化脚本</h2>
<p>为了降低成本，您可以按需增加和配置资源——因此您只需为您使用的资源付费。</p>
<p>让我们编写几个脚本，它们将:</p>
<ol>
<li>用Docker机器提供液滴</li>
<li>配置Docker群组模式</li>
<li>向群集添加节点</li>
<li>展开火花</li>
<li>进行火花作业</li>
<li>完成后，旋转水滴</li>
</ol>
<p><em> create.sh </em>:</p>
<div class="codehilite"><pre><span/><code><span class="ch">#!/bin/bash</span>


<span class="nb">echo</span> <span class="s2">"Spinning up three droplets..."</span>

<span class="k">for</span> i <span class="k">in</span> <span class="m">1</span> <span class="m">2</span> <span class="m">3</span><span class="p">;</span> <span class="k">do</span>
  docker-machine create <span class="se">\</span>
    --driver digitalocean <span class="se">\</span>
    --digitalocean-access-token <span class="nv">$DIGITAL_OCEAN_ACCESS_TOKEN</span> <span class="se">\</span>
    --engine-install-url <span class="s2">"https://releases.rancher.com/install-docker/19.03.9.sh"</span> <span class="se">\</span>
    node-<span class="nv">$i</span><span class="p">;</span>
<span class="k">done</span>


<span class="nb">echo</span> <span class="s2">"Initializing Swarm mode..."</span>

docker-machine ssh node-1 -- docker swarm init --advertise-addr <span class="k">$(</span>docker-machine ip node-1<span class="k">)</span>

docker-machine ssh node-1 -- docker node update --availability drain node-1


<span class="nb">echo</span> <span class="s2">"Adding the nodes to the Swarm..."</span>

<span class="nv">TOKEN</span><span class="o">=</span><span class="sb">`</span>docker-machine ssh node-1 docker swarm join-token worker <span class="p">|</span> grep token <span class="p">|</span> awk <span class="s1">'{ print $5 }'</span><span class="sb">`</span>

docker-machine ssh node-2 <span class="s2">"docker swarm join --token </span><span class="si">${</span><span class="nv">TOKEN</span><span class="si">}</span><span class="s2"> </span><span class="k">$(</span>docker-machine ip node-1<span class="k">)</span><span class="s2">:2377"</span>
docker-machine ssh node-3 <span class="s2">"docker swarm join --token </span><span class="si">${</span><span class="nv">TOKEN</span><span class="si">}</span><span class="s2"> </span><span class="k">$(</span>docker-machine ip node-1<span class="k">)</span><span class="s2">:2377"</span>


<span class="nb">echo</span> <span class="s2">"Deploying Spark..."</span>

<span class="nb">eval</span> <span class="k">$(</span>docker-machine env node-1<span class="k">)</span>
<span class="nb">export</span> <span class="nv">EXTERNAL_IP</span><span class="o">=</span><span class="k">$(</span>docker-machine ip node-2<span class="k">)</span>
docker stack deploy --compose-file<span class="o">=</span>docker-compose.yml spark
docker service scale <span class="nv">spark_worker</span><span class="o">=</span><span class="m">2</span>


<span class="nb">echo</span> <span class="s2">"Get address..."</span>

<span class="nv">NODE</span><span class="o">=</span><span class="k">$(</span>docker service ps --format <span class="s2">"{{.Node}}"</span> spark_master<span class="k">)</span>
docker-machine ip <span class="nv">$NODE</span>
</code></pre></div>

<p><em> run.sh </em>:</p>
<div class="codehilite"><pre><span/><code><span class="ch">#!/bin/sh</span>

<span class="nb">echo</span> <span class="s2">"Getting container ID of the Spark master..."</span>

<span class="nb">eval</span> <span class="k">$(</span>docker-machine env node-1<span class="k">)</span>
<span class="nv">NODE</span><span class="o">=</span><span class="k">$(</span>docker service ps --format <span class="s2">"{{.Node}}"</span> spark_master<span class="k">)</span>
<span class="nb">eval</span> <span class="k">$(</span>docker-machine env <span class="nv">$NODE</span><span class="k">)</span>
<span class="nv">CONTAINER_ID</span><span class="o">=</span><span class="k">$(</span>docker ps --filter <span class="nv">name</span><span class="o">=</span>master --format <span class="s2">"{{.ID}}"</span><span class="k">)</span>


<span class="nb">echo</span> <span class="s2">"Copying count.py script to the Spark master..."</span>

docker cp count.py <span class="nv">$CONTAINER_ID</span>:/tmp


<span class="nb">echo</span> <span class="s2">"Running Spark job..."</span>

docker <span class="nb">exec</span> <span class="nv">$CONTAINER_ID</span> <span class="se">\</span>
  bin/spark-submit <span class="se">\</span>
    --master spark://master:7077 <span class="se">\</span>
    --class endpoint <span class="se">\</span>
    /tmp/count.py
</code></pre></div>

<p><em> destroy.sh </em>:</p>
<div class="codehilite"><pre><span/><code><span class="ch">#!/bin/bash</span>

docker-machine rm node-1 node-2 node-3 -y
</code></pre></div>

<p>测试一下！</p>
<hr/>
<p>代码可以在<a href="https://github.com/testdrivenio/spark-docker-swarm"> spark-docker-swarm </a> repo中找到。干杯！</p>
  </div>

  </div>    
</body>
</html>